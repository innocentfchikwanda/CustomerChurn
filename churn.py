# -*- coding: utf-8 -*-
"""churn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QFbiAgTTHttGTXPrACPmxkIU_6VlzPIK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

from google.colab import drive
drive.mount("/content/drive/")

data = pd.read_csv("/content/drive/My Drive/Colab Notebooks/CustomerChurn_dataset.csv")

customers = data.copy()

"""Exploratory Data Analysis"""

#Exploratory Data Analysis

customers.head(3)

customers.info()

customers["SeniorCitizen"] = customers["SeniorCitizen"].astype("category")

customers["TotalCharges"] = customers["TotalCharges"].replace(" ", 0)

customers["TotalCharges"] = customers["TotalCharges"].astype("float")

customers = customers.iloc[:,1:]

customers.info()

for column in customers.columns:
    unique_categories = customers[column].unique()
    print(f"Categories in {column}:\n{unique_categories}\n")

from sklearn.preprocessing import LabelEncoder, StandardScaler
label_encoder = LabelEncoder()
customers['Churn'] = label_encoder.fit_transform(customers['Churn'])

Y = customers["Churn"]

numeric = customers.iloc[:,17:19]

numeric["tenure"] = customers["tenure"]

non_numeric = customers.drop(["tenure", "MonthlyCharges", "TotalCharges"], axis = 1)

non_numeric.head(2)

sns.set(rc={'figure.figsize':(11.7,8.27)})
i = 0
for k in range(1,5):
    plt.subplot(220 + k)
    col = non_numeric.keys()[i]
    sns.countplot(x=col, hue='Churn', data=non_numeric)
    i += 1

for k in range(1,3):
    plt.subplot(230 + k)
    col = non_numeric.keys()[i]
    sns.countplot(x=col, hue='Churn', data=non_numeric)
    i += 1

for k in range(1,5,2):
    plt.subplot(240 + k)
    col = non_numeric.keys()[i]
    sns.countplot(x=col, hue='Churn', data=non_numeric)
    i += 1

"""Data Preparation"""

non_numeric = non_numeric.drop(["Churn"], axis=1)

cat_cols = non_numeric.select_dtypes(include=['object']).columns

non_numeric = pd.get_dummies(non_numeric, columns=cat_cols)

non_numeric.head(20)

X = pd.concat([non_numeric, numeric], axis=1)

X.head(2)

scaler = StandardScaler()

scaled = scaler.fit_transform(X)

# Create a new DataFrame with the scaled data
X = pd.DataFrame(scaled, columns=X.columns)

"""Feature Selection"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier()

clf.fit(X_train, y_train)

feature_importances = clf.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

print(feature_importance_df)

X = X.drop(["TechSupport_No internet service", "Contract_Two year", "StreamingMovies_No internet service", "PhoneService_No", "OnlineBackup_No internet service", "OnlineSecurity_No internet service", "InternetService_No", "InternetService_DSL", "DeviceProtection_No internet service"], axis=1)

predictors = [i for i in X.keys()]

predictors

"""Model Building"""

import keras
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from keras.utils import to_categorical

!pip install tensorflow scikeras scikit-learn

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Keras Functional API model
input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(12, activation='relu')(hidden_layer_2)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))

_, accuracy = model.evaluate(X_train, y_train)
accuracy*100

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

"""Deploying Model"""

import pickle

# Assuming 'model' is your trained model
with open('churn_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)